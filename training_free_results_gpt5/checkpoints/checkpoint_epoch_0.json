{
  "epoch": 0,
  "experiences": {
    "0": "Establish a transparent, reproducible EO pipeline for multi-year hotspot analysis: 1) Inventory and publish all inputs with absolute file paths, dates, sources, and versions; filter files by AOI/city and month before aggregation. 2) Verify units (e.g., radiance vs DN), masks (cloud/shadow/nodata), and consistent AOI, projection, resolution, and extent across all months and years. 3) Build explicit temporal aggregation: compute per-month mean radiance using only valid pixels (apply nodata/cloud/shadow masks); record monthly means, valid pixel counts, and coverage fractions. 4) Aggregate months into yearly composites using a consistent statistic (mean or median) across years; for mean aggregation, weight each month by its count of valid pixels; document weights and any months excluded due to low coverage. 5) Compute yearly mean radiance and explicitly set and report the hotspot threshold (e.g., threshold = mean × 1.5), documenting the rationale. 6) Compute inter-year change using difference-of-means; optionally compute mean-of-differences as a cross-check and reconcile any discrepancies (note effects of weighting and varying monthly coverage). 7) Calculate hotspot proportions correctly (convert stated percentages to proportions, track pixel counts, and use only valid pixels). 8) Quantify and justify differences across years (document rounding choices, uncertainty metrics, monthly/yearly valid-pixel counts, and any processing changes). 9) Interpret development patterns and, when possible, cross-validate with ancillary datasets (e.g., built volume or nightlights from other sources) to strengthen conclusions. 10) Ensure end-to-end reproducibility by documenting all intermediate values (monthly means, weights, thresholds) alongside exact tool outputs and paths.",
    "1": "Reproducible Landsat OLI/TIRS single-channel LST workflow for class-wise statistics, proportions, and scene aggregation: 0) Read and log product metadata upfront: parse the USGS MTL file and raster tags to confirm product level (e.g., Level-1 DN/TOA vs Level-2 Surface Temperature), units, scale factors for surface reflectance bands, and thermal calibration constants (ML/AL and K1/K2); record sensor/platform, processing dates, and any scene-specific flags. 1) Inventory and log all inputs with absolute file paths, product names/versions, sensor/platform, and processing dates; clip all inputs to the AOI and record the AOI definition. 2) Rigorous preprocessing before any thresholding: apply QA-based cloud/shadow/snow/ice masking (QA_PIXEL/BQA) and a reliable water mask; preserve a per-scene valid-data mask; explicitly exclude nodata/fill and track valid coverage fractions. 3) Validate inputs and units and apply the correct thermal chain when needed: confirm OLI Bands 4 (Red) and 5 (NIR) are surface reflectance (document scale factors/units), and that the thermal input (TIRS Band 10) is brightness temperature in Kelvin; if Level-1 DN or radiance, convert DN to TOA radiance (L = ML × DN + AL), then to brightness temperature (BT = K2 / ln(K1/L + 1)), and compute LST with an NDVI-based emissivity correction; if Level-2 Surface Temperature is available, use it as a benchmark or primary product and document the choice. 4) Harmonize spatial properties: choose a target grid, reproject as needed, resample, and co-register OLI and TIRS to ensure pixel alignment; document the resampling method and verify alignment. 5) Compute NDVI from surface reflectance (OLI: Band 5 NIR, Band 4 Red) for emissivity estimation and class masking; document NDVI scaling and any thresholds used. 6) Derive emissivity via an NDVI-based method (e.g., NDVI thresholds or fractional vegetation emissivity); document the exact method, constants, and definitions. 7) Run single-channel LST: use TIRS Band 10 brightness temperature with the NDVI-based emissivity correction to derive LST; verify the output units are Kelvin and record all parameters; note that single-channel is the default for Landsat TIRS (split-window is not applicable). 8) Validate outputs before thresholding: inspect quick histograms and summary statistics for NDVI, emissivity, BT, and LST over valid land pixels; check plausible ranges and unit consistency; cross-check LST against independent products (e.g., USGS Level-2 Surface Temperature or MODIS LST) and flag/analyze discrepancies. 9) Define and validate class masks based on NDVI or land cover (e.g., NDVI > 0.7 forest; NDVI < 0.2 non-vegetated); adjust thresholds per AOI and document choices. 10) Compute proportions and class-wise statistics using only valid land pixels (water excluded): set clear thresholds/operators, calculate numerators (pixels meeting criteria) and denominators (total valid land pixels), and report proportions with pixel counts and uncertainty metrics (standard deviation or confidence intervals). 11) Aggregate across scenes/dates using masked per-scene means (or medians) weighted by valid pixel counts; track valid coverage fractions, flag low-confidence aggregates, and consider robust aggregators to reduce outlier influence. 12) Report signed temperature differences to avoid ambiguity (e.g., forest cooler than urban by ~9 K). 13) Ensure end-to-end reproducibility: pass exact file paths between stages, log CRS/resampling choices, QA/water masks applied, thresholds, algorithms/constants (including ML/AL and K1/K2), tool versions, and any processing changes; preserve per-scene valid-data masks and decision trails.",
    "2": "Full, chained water-mapping workflow emphasizing reproducibility: 1) Inventory scenes for the AOI/timeframe and retain metadata plus absolute file paths for all products (surface reflectance, QA). 2) Apply QA cloud and shadow masks per sensor (e.g., Landsat Pixel QA/BQA; Sentinel-2 QA60/SCL), optionally refine with cloud probability and simple morphological operations; carry forward a valid-data mask per scene. 3) Compute NDWI on surface reflectance: Landsat 5/7 (Green B2 vs NIR B4), Landsat 8/9 (Green B3 vs NIR B5), Sentinel-2 (Green B3 vs NIR B8); consider MNDWI (Green vs SWIR1) when water–built-up separation is poor. 4) Choose and justify a water threshold: start with NDWI > 0 as baseline; preferably derive an AOI-specific threshold via Otsu or percentile analysis on clear pixels; document method and parameters. 5) Generate cleaned water masks: apply the threshold only on valid pixels, remove speckle/small fragments, fill small holes, and optionally constrain by slope or persistent-water priors; preserve nodata where clouds/shadows were masked. 6) Compute water area per date within the AOI using pixel area and the valid-data mask; track coverage fraction to flag low-confidence dates. 7) Establish a clear baseline (e.g., the clearest image with maximum water area, or a median across multiple clear dates) and compute relative loss per date versus this baseline. 8) Detect peak loss by scanning the time series; validate peaks against residual cloud/shadow contamination (e.g., high masked fraction) and potential threshold drift (e.g., NDWI histogram shifts); mark unreliable peaks and, if needed, re-run with adjusted masks/thresholds. 9) Chain all steps via the returned file paths from each stage for end-to-end reproducibility; avoid guessing thresholds or manual edits without documented rationale.",
    "3": "Rigorous AOI-driven TVDI hotspot workflow (with exact date/product matching and AOI-specific wet/dry edges): 1) Inventory and match exact inputs to the request: log absolute file paths, product names/versions, sensor/platform, and acquisition date/time; ensure the specific date and products requested are selected (e.g., MODIS LST and NDVI/EVI for 2022-07-12); record the AOI definition (urban boundary, agricultural/rice mask, and any buffer). 2) Clip all inputs to the AOI; optionally define a buffer for surroundings and keep its boundary explicit. 3) Apply sensor-specific QA/cloud/shadow masking (e.g., MODIS QA/state bits) and exclude water with a reliable water mask; carry forward a per-scene valid-data mask and track the AOI valid coverage fraction. 4) Harmonize spatial properties: reproject to an appropriate equal-area CRS for area calculations; co-register and align NDVI/EVI and LST to the same grid, resolution, and extent; handle nodata consistently and document how it propagates. 5) Prepare vegetation and thermal indices: use NDVI as the default for TVDI; if EVI is used due to NDVI saturation or product availability, document the choice, scaling, and rationale. 6) Compute TVDI using robust wet/dry edge fitting derived from valid AOI pixels (fit Tmin(NDVI/EVI) and Tmax(NDVI/EVI)); verify TVDI is bounded within [0–1] and inspect its distribution for anomalies. 7) Define the hotspot threshold clearly (e.g., TVDI > 0.75) and state operator strictness ('>' vs '≥'); compute area-weighted percentages using pixel areas in the equal-area CRS and only valid AOI pixels; for agricultural analyses, report the percentage of valid agricultural pixels exceeding the threshold. 8) Map hotspots (threshold mask) and summarize with zonal statistics for defined subzones (urban core, buffer, or agricultural regions); report pixel counts and coverage fractions. 9) Ensure end-to-end reproducibility: log absolute file paths, exact dates/products selected, AOI definitions, CRS/resampling/co-registration choices, QA/water masks applied, wet/dry edge-fitting parameters, thresholds/operators, and any processing changes.",
    "4": "Ensemble, ontology-aligned classifier with explicit decision rules, uncertainty gating, terrain/context checks, and reproducible mapping: 1) Inventory all inputs and retain absolute file paths, dates, sources, versions, model checkpoints; log code versions, configuration files, and random seeds. 2) Define the target ontology (label set) and confirm label-name parity and class definitions across all models; ensure the primary classifier is trained for or mapped to this ontology; keep documented lookup/mapping tables to the ontology. 3) Run at least two complementary scene classifiers per image (e.g., a multispectral CNN tuned for remote sensing such as MSCN, and a vision–language model such as RemoteCLIP; also acceptable: a CNN image classifier, CLIP/vision-language model, and/or a multispectral RF/GBM); record per-model per-class scores (logits/probabilities) and top-k predictions. 4) Normalize all model outputs to the common ontology before any fusion; if needed, calibrate probabilities (e.g., temperature scaling or Platt scaling) so confidences are comparable; document calibration data, methods, and parameters. 5) Harmonize score scales and define a fusion scheme (e.g., weighted averaging of calibrated probabilities, rank-based voting); document model weights and fusion rules. 6) Set primary decision rules: require a minimum top-1 confidence threshold t (e.g., calibrated 0.7–0.8) and/or a margin over the second-best delta; optionally require cross-model agreement on top-1. Include an explicit combination rule: either both models predict the target class with confidence ≥ t, or one model predicts the target class with very high confidence ≥ t_high while the other lists the target in its top-5 with supportive confidence ≥ t_support. Document t, t_high, t_support, delta, and agreement criteria. 7) Apply uncertainty gating: flag predictions as ambiguous when top-1 < t, margin < delta, cross-model disagreement occurs, or prediction entropy exceeds a threshold; inspect top-5 across models for these cases and record the conditions that triggered gating. 8) Maintain and update confusion sets (e.g., Beach vs Desert vs Port vs Pond; Farmland vs Meadow/Park; BareLand vs Desert vs Beach; Urban vs Industrial; Wetland vs Water; Mountain vs Hill vs Plateau) based on validation; use them to trigger secondary checks. 9) For flagged/ambiguous cases, compute domain-informed features from multispectral or ancillary data: NDVI/EVI, NDWI (water presence), NDBI (built-up), brightness/albedo (bright sand), SWIR reflectance, simple GLCM texture metrics, field-boundary/row periodicity indicators; add terrain metrics where relevant (DEM-derived slope, ruggedness/terrain roughness, relief/elevation ranges). 10) Apply tie-breaker rules using spatial/context features: shoreline adjacency for Beach (distance to coastline/water mask), dune-like textures and broad homogeneity for Desert (low NDVI, high brightness/albedo, uniform texture), proximity to port infrastructure and quay geometry for Port (high NDBI, linear man-made structures, water adjacency), pond indicators (small enclosed water bodies, strong NDWI, homogeneous texture), proximity to infrastructure and heterogeneous bare soil for BareLand, farmland indicators (rectangular parcel geometry, row patterns, moderate NDVI variability), meadow/park indicators (higher NDVI, homogeneous texture, urban adjacency); for terrain-dependent classes, require DEM-based checks (e.g., Mountain: high slope/ruggedness above elevation thresholds; Plateau: high elevation with low slope; Hill: moderate slope/ruggedness). 11) Automate the final tally: count a class only when the fused top-1 is above t (or passes the combination rule with t_high/t_support) and margin ≥ delta (and passes any confusion tie-breakers); otherwise flag the item as low-confidence and route to a manual review queue. 12) Record per-image decision trails: candidate labels, per-model confidences/scores, top-5 lists, mapped labels, confusion flags, tie-breaker feature values and outcomes (including DEM-derived metrics when used), fusion outputs, final label, with references to all input/output paths. 13) Programmatically aggregate results across images and time windows; produce transparent counts and summaries by class and confidence band; validate against holdout/trusted labels; maintain a confusion matrix to quantify errors; update thresholds, model weights, calibration, and confusion sets accordingly; document all changes to maintain auditability. 14) Ensure end-to-end reproducibility by chaining outputs via returned file paths, retaining ontology mappings, thresholds, calibration artifacts, and configuration files; keep runs deterministic with fixed seeds; avoid ad hoc edits without documented rationale.",
    "5": "Reproducible ASTER TIR TES urban thermal workflow (with explicit Δε derivation and validation): 1) Verify ASTER TIR inputs (Bands 10–14) and metadata; retain absolute file paths, product versions, and processing dates in a log. 2) Run TES to derive LST and an emissivity stack (bands 10–14); confirm LST units are Kelvin and validate emissivity scaling (0–1 vs scaled to 0–100 or 0–1000); document the emissivity definition and tool versions. 3) Clip LST and emissivity outputs to the specified AOI. 4) Apply an explicit urban/built-up mask from a reliable land cover product (e.g., NLCD, GHSL, ESA WorldCover), reprojected and aligned to the TES grid; document reprojection/resampling choices and verify pixel alignment. 5) Add QA/cloud/water masks to exclude contaminated pixels; carry forward a valid-data mask per scene and preserve nodata. 6) Define and document emissivity metrics for selection: compute per-pixel Δε = max(emissivity[10–14]) − min(emissivity[10–14]) from the emissivity stack; confirm emissivity scaling and adjust Δε thresholds accordingly (e.g., 0.05 in 0–1 units equals 5 if 0–100 scaling); optionally compute additional metrics (mean across bands or a specific-band emissivity); record the exact bands used. 7) Thresholding and proportions: either compute a Δε raster first and apply single-band thresholding, or configure a multi-band threshold tool for a \"range\" across emissivity bands to implement Δε; compute and report the denominator (total valid urban pixels) and the numerator(s) for the chosen criteria (e.g., urban pixels with LST > 300 K and emissivity < 0.96, and/or with Δε > threshold); calculate final percentages using only valid pixels. 8) Sanity checks before reporting: generate summary statistics and histograms for LST, emissivity, and Δε over valid pixels; render a quick QA map to catch scaling or masking issues; track coverage fraction and flag low-confidence results. 9) Ensure end-to-end reproducibility: pass exact file paths between steps, maintain AOI/CRS/resolution/extent, check CRS and pixel alignment at each stage, document parameters and any processing changes, and preserve a decision trail for thresholds, scaling, and masks.",
    "6": "MODIS absorption-peak workflow for coastal ROI with correct peak aggregation: 1) Constrain all processing to the intended ROI (e.g., Guangdong coast) and maintain a processing log with absolute file paths, product names (Terra/Aqua), collection/version, and dates. 2) Apply MODIS QA/cloud masking before any index computation: use MODIS Surface Reflectance QA/state bits (e.g., MxD09GA/MxD09GQ) and/or MOD35/MYD35 cloud mask to remove clouds, cloud shadows, heavy aerosol, sunglint/bright water, and other invalid pixels; carry forward a valid-data mask per scene and track ROI coverage. 3) Compute a transparent absorption index on surface reflectance: A = 1 − Rabs/Rcont, where Rabs is from absorption bands (e.g., MODIS bands 17–19) and Rcont from continuum bands (e.g., bands 02 and/or 05); document the exact formula, bands used, and any coefficients, and calibrate/normalize appropriately if reporting physical units or comparing across dates. 4) Derive the per-scene ROI peak using only valid pixels: compute the ROI maximum (or a robust high percentile such as the 99th) for each date and record pixel counts and coverage fraction; do not take the mean of maxima to define the period peak. 5) Compute the period peak as the temporal maximum of the per-scene ROI maxima (or chosen robust percentile) across the analysis window; report the timestamp (date/time, sensor) and the peak magnitude, and note coverage constraints if the valid-data fraction is low. 6) Cross-check the peak’s magnitude and timing with an independent product (e.g., MOD05/MYD05 water vapor or ERA5 total column water vapor/related atmospheric metric) to validate plausibility; flag discrepancies and revisit QA/masking or index parameters if needed. 7) Ensure end-to-end reproducibility: preserve AOI, projection, resolution, and extent across scenes; document any resampling/alignment choices, and chain all steps via returned file paths from each stage.",
    "7": "Robust EO time series and trend fitting workflow (with strict period/AOI filtering, variable alignment, unit clarity, city-to-city comparability, and explicit pre-change metadata/aggregation checks): 1) Enumerate inputs via a deterministic file listing (e.g., get_filelist), strictly filter to the exact requested period and city AOI, and log absolute paths, product names, sensor/platform, collection/version, and processing dates; parse the acquisition year from filenames or metadata, explicitly sort chronologically, and note/handle duplicates or missing years. Always align the dataset with the requested variable: select an appropriate product for the phenomenon (e.g., precipitation: IMERG/ERA5; urban heat: LST), rather than repurposing unrelated indices (e.g., do not use NDVI for precipitation). 2) Constrain all processing to the AOI and harmonize spatial properties (CRS, grid, resolution, extent); carry forward a per-date valid-data mask and apply product-specific QA/quality filters (e.g., NoData/fill, clouds, cloud shadows, snow/ice, water mask, VIIRS stray-light/QA flags) while tracking the valid coverage fraction. For gridded climate products (e.g., IMERG/ERA5), apply their QA flags and verify scaling/units before statistics. 3) Inspect dataset metadata before any aggregation/change: explicitly load and examine variable names, units, scale factors, nodata/fill values, temporal indexing/calendar, and spatial nature (gridded field vs pre-aggregated city totals). Determine whether values are already city-/region-level totals/means; if gridded, plan zonal statistics over the AOI with proper masks and document any required unit/area scaling (e.g., density per km², pixel-area conversions). 4) Compute per-date statistics using only valid pixels and include robust measures to reduce bias from outliers: masked per-image spatial mean (primary), median, and/or a trimmed mean (e.g., 10% trim). Ensure the statistic matches the variable—e.g., precipitation as AOI spatial mean in mm/time (and, if aggregating, use sums for totals and means for rates); document chosen statistics, thresholds, scaling factors, units, and pixel counts for each date. 5) Build the time series with actual calendar years (or months/days) as x-values: if multiple scenes exist per year, aggregate with a consistent, documented rule (e.g., mean or median weighted by valid pixel counts); for precipitation, define and document the temporal aggregation (daily→monthly totals, monthly→annual totals or means), record scene counts per AOI/timeframe and coverage-driven confidence flags. 6) Fit trends using both parametric and non-parametric methods: perform OLS (and, if needed, robust regression) and report diagnostics—slope, intercept, R², p-values, standard errors, confidence intervals. Also run a Mann–Kendall (MK) trend test and report Kendall’s tau and its p-value; optionally report Sen’s slope. Avoid conclusions based solely on a positive OLS slope—require statistical significance and reconcile OLS and MK outcomes; note assumptions (autocorrelation, seasonality) and address them where justified (e.g., seasonal MK, prewhitening). 7) Use ancillary layers to ensure the statistic reflects the intended phenomenon (e.g., city lights or land surface temperature); apply built-up/urban masks (GHSL, ESA WorldCover, NLCD) and exclude water/rural areas when appropriate to the variable. For precipitation, ensure the AOI definition is appropriate and be explicit if including/excluding water areas. Assess sensitivity of the trend to mask/AOI choice and document impacts. 8) Explicitly state the physical variable and units: distinguish brightness temperature (BT) from true LST, precipitation rates vs totals, reflectance vs indices; verify inputs/outputs are in their correct units (e.g., Kelvin, mm/day, mm/month) and, if converting (e.g., K→°C), do so transparently and document the conversion; retain native units in stored outputs to avoid confusion. 9) Exact-change extraction for specific years and rounding: for requested years (e.g., 1985 and 2020), extract exact AOI values from the time series (or pre-aggregated city totals if provided); compute both absolute change (Δ = value2020 − value1985) and percentage change (%Δ = 100 × Δ / value1985) using consistent rounding rules; verify that both metrics align with the dataset’s units and any provided options. If one metric matches but the other does not, investigate unit/scaling mismatches (e.g., scale factors, per-km² vs absolute totals, density-to-total conversions) rather than selecting the closest percentage; adjust calculations only with documented rationale. 10) Report comparability metrics for city-to-city conclusions: scene/date counts analyzed, valid coverage fractions, and basic variability (e.g., standard deviation, interquartile range) for each city/period to support robust comparisons; ensure consistent processing, masks, thresholds, scaling, and aggregation rules across cities/regions. 11) Cross-validate the time series and fitted trend against independent datasets appropriate to the variable (e.g., DMSP-OLS or alternate VIIRS products for lights; MODIS LST or ERA5 for temperature; IMERG vs ERA5 vs CPC vs gauge networks for precipitation) to confirm plausibility; investigate and document discrepancies. 12) Ensure end-to-end reproducibility: retain AOI definitions, CRS/resampling choices, QA flags and thresholds, time series tables (date/year, statistic, valid pixel count, coverage fraction, scene counts, variability), regression and MK configurations/outputs (model files, diagnostics, tau/Sen’s slope), unit conversions, change computations (absolute and percentage with rounding rules), and code/version logs; avoid ad hoc edits without rationale.",
    "8": "Reproducible split-window LST workflow for dual-thermal-band sensors (e.g., MODIS Bands 31/32, AVHRR): 1) Inventory all inputs and maintain a processing log with absolute file paths, product names/versions, acquisition dates/times, sensor/platform, and any ancillary layers (emissivity maps, land cover/irrigation masks); record code/tool versions and configuration. 2) Preprocess and verify units: confirm thermal inputs are brightness temperatures (BT) for the relevant bands (e.g., BT31/BT32) in Kelvin; if inputs are radiance or DN, convert to BT using the correct calibration constants; verify emissivity inputs (source, definition, band-specific or broadband) and document how emissivity will be estimated (NDVI-based, land-cover lookup, ASTER GED, or product-provided). 3) Harmonize spatial properties: reproject to an appropriate equal-area CRS for area calculations; co-register and align BT31, BT32, emissivity layers, and QA masks to a common grid, resolution, and extent; document resampling methods and check pixel alignment. 4) Apply QA/cloud and ancillary masks before any LST computation: use sensor-specific QA/state bits to remove clouds, cloud shadows, heavy aerosol, snow/ice, and fill; exclude water pixels with a reliable water mask; optionally apply an irrigated-farmland mask if the analysis requires isolating non-irrigated surfaces; carry forward a per-pixel valid-data mask and track AOI coverage fraction per scene. 5) Compute LST with sensor-appropriate split-window coefficients: use published split-window algorithms for the sensor (coefficients depend on BT31/BT32, emissivity, emissivity difference, view zenith, and atmospheric water vapor); document the exact formula variant, coefficient values, inputs (BT31/BT32, emissivity, Δemissivity, view angle, atmospheric parameters), and tool versions; ensure the output LST units are Kelvin. 6) Validate outputs: check plausible LST ranges and distributions (e.g., 250–330 K depending on context), inspect histograms and BT–LST scatter for anomalies, and, when available, cross-check against independent LST products (e.g., MOD11/MYD11 or MOD21/MYD21) or in-situ proxies; flag and investigate discrepancies (unit issues, misalignment, QA leakage). 7) Classify LST into defined bins and compute area-weighted percentages: apply bin thresholds on valid pixels only, compute per-bin areas using pixel area in the equal-area CRS, and report percentages that sum to ~100% (accounting for excluded/invalid pixels); document bin definitions, thresholds, and per-bin pixel counts. 8) Optional temporal aggregation: if summarizing multiple scenes/dates, aggregate bin counts or LST statistics using only valid pixels and weight by valid coverage; report coverage fractions and uncertainty metrics. 9) Ensure end-to-end reproducibility: retain and pass exact file paths between stages, log AOI definitions, CRS/resampling choices, split-window coefficients/parameters, emissivity method, QA masks applied, and any processing changes so results can be fully replicated.",
    "9": "Reproducible NDVI/EVI hotspot coverage and change workflow: 1) Define the metric unambiguously: hotspot (vegetation) coverage = fraction of valid pixels with NDVI (or EVI) above a chosen threshold; document the operator ('>' vs '≥'), the index used (NDVI primary; EVI for cross-checks), and index scaling (float −1..1 vs int16 with scale factor). 2) Inventory NDVI/EVI rasters via a deterministic file listing and log absolute file paths, dates/timestamps, sensors/collections/versions, and processing dates; constrain to the AOI and record its definition. 3) Validate the index product and scaling: confirm it is NDVI/EVI (not raw bands), verify scaling and projection; if not available, compute NDVI/EVI from surface reflectance with sensor-appropriate bands (e.g., Landsat 8/9: NDVI = B5/B4; EVI uses B2, B4, B5 with documented coefficients) and document algorithms, constants, and tool versions. 4) Clip to the AOI and harmonize spatial properties: use an equal-area CRS for area calculations; co-register and align all dates to a common grid, resolution, and extent; correct scaling and sanity-check plausible value ranges; ensure co-registration before any per-pixel comparisons. 5) Apply QA/cloud/snow masks appropriate to the sensor (e.g., Landsat QA_PIXEL/BQA; Sentinel-2 QA60/SCL); optionally exclude water using a reliable water mask; carry forward a valid-data mask per date; track the valid coverage fraction. 6) Select a biologically meaningful threshold: derive AOI-specific NDVI/EVI thresholds using histograms/percentiles (e.g., Otsu or a chosen percentile on clear land pixels) and local knowledge; validate against land cover and expected vegetation states; document the method, parameters, and the strictness of the operator. 7) Compute per-date coverage: threshold on valid pixels only; count the numerator (hotspot/vegetated pixels) and denominator (total valid pixels) and derive area fractions using pixel areas in the equal-area CRS; record pixel counts and masked/valid coverage; perform sanity checks for implausible values (e.g., ~0% or ~100%) and investigate mask leakage, scaling errors, or AOI issues. 8) Aggregate to exact query windows: group dates into the user/requested windows; compute window-level coverage using a consistent rule (e.g., weighted mean by valid pixel counts or median); record weights, window membership, excluded dates and reasons (low coverage), and coverage fractions per window; optionally compute percent change between windows using area fractions. 9) Cross-checks and change maps: compare NDVI-based coverage with EVI-based coverage; inspect NDVI/EVI histograms per date/window; generate NDVI/EVI difference maps to verify spatial consistency of hotspot changes; adjust thresholds if justified by cross-checks and document the rationale. 10) Ensure end-to-end reproducibility: retain AOI definitions, CRS/resampling choices, threshold derivation and values, scaling factors, pixel counts, coverage fractions, window aggregates, percentage changes, and exact input/output file/date paths; chain steps via saved paths and flag low-confidence outputs when valid-data coverage is insufficient.",
    "10": "Reproducible MODIS split-window LST workflow with AOI/QA masking and justified scene selection: 1) Inventory inputs via get_filelist and log absolute file paths, sensor (Terra/Aqua), collection/version, acquisition date/time, and AOI coverage; ensure BT31/BT32 and an emissivity source are available for the target date. 2) Explicitly choose the acquisition time consistent with the question’s intent (e.g., daytime for urban heat); document the choice (Terra ~10:30 local, Aqua ~13:30) and rationale; confirm sufficient valid coverage over the AOI (e.g., central Wuhan). 3) Clip all inputs (BT31, BT32, emissivity) to the AOI; harmonize CRS, resolution, and grid alignment across bands; record any resampling method/parameters. 4) Apply MODIS QA/cloud and water/invalid masking before LST computation: use MOD35/MYD35 or SR QA/state bits to remove clouds, shadows, heavy aerosol, and other invalid pixels; exclude water with a reliable water mask; preserve a per-scene valid-data mask and log coverage fraction. 5) Run split_window using BT31/BT32 and emissivity to derive LST; document the exact formula, coefficients, and tool version; verify input BT units and confirm the LST output units are Kelvin; sanity-check expected ranges. 6) Compute proportions on the same masked LST raster: define clear thresholds (e.g., strictly >310 K and strictly <295 K); calculate numerators (count of valid AOI pixels meeting each threshold) and the denominator (total valid AOI pixels); report proportions with pixel counts and the timestamp to avoid mixing results. 7) Maintain traceability end-to-end: pass full saved paths between stages, label thresholds and timestamps in outputs, and avoid selecting outputs merely because they match answer options; record AOI definitions, CRS, masks, resampling choices, and any processing changes. 8) If multiple scenes are analyzed, optionally aggregate with weights based on valid pixel counts; cross-check LST magnitudes against independent products (e.g., MYD11/MOD11 LST or ERA5) and flag discrepancies.",
    "11": "Reproducible per-pixel turbidity change workflow with threshold-based spatial criteria: 1) Inventory all inputs for the lake AOI/timeframe and retain absolute file paths, product names, sensor/collection versions, and processing dates in a log; harmonize CRS, resolution, extent, and pixel alignment across dates. 2) Explicitly define the change reference: choose consecutive-date comparisons (t vs t−1) or a fixed baseline (e.g., clearest baseline date or a multi-date median); justify and document the choice, baseline date(s), and file paths. 3) Compute turbidity rasters per date from consistent inputs (e.g., surface reflectance-based turbidity proxy or calibrated NTU retrieval); document the exact formula, bands, coefficients, units, and tool versions; ensure consistent calibration across dates. 4) Apply a consistent lake water mask for all dates (e.g., NDWI/MNDWI-derived or a stable lake polygon) aligned to the turbidity grid; exclude non-water and preserve nodata. 5) Apply sensor-specific QA/quality masks per date to remove clouds, cloud shadows, bright water/sunglint, heavy aerosol, and fill; carry forward a per-date valid-data mask. 6) For each comparison, intersect the valid-data masks across the two dates to obtain paired-valid lake pixels and track the coverage fraction. 7) Compute per-pixel percent change only on paired-valid lake pixels where the reference turbidity > 0: percent_change = 100 × (current − reference) / reference; document handling of zero/near-zero reference values and any minimum thresholds to avoid division artifacts. 8) Threshold severe change with an explicit operator (e.g., strict ‘>’ 30% or inclusive ‘≥’ 30%) to produce a binary severe-change mask; save the mask with exact paths. 9) Derive proportions via pixel counts over paired-valid lake pixels: numerator = count of severe pixels; denominator = count of paired-valid lake pixels; proportion = numerator / denominator; report pixel counts and coverage fractions. 10) Scan the analysis window and select the date/comparison with the maximum severe-change proportion; record the timestamp(s), proportion, and coverage fraction. Do not substitute scene-wide means for area-based severe-change metrics. 11) Cross-validate severe-change findings against independent data (e.g., in-situ turbidity, Secchi depth, or other water-quality products) and investigate discrepancies; document uncertainties and any parameter adjustments. 12) Ensure end-to-end reproducibility by chaining outputs via returned file paths and documenting AOI definitions, CRS/resampling choices, masks, thresholds, baseline selection, formulas, and any processing changes.",
    "12": "Reproducible ASTER TIR TTM urban–rural UHII workflow with strict AOI/QC, domain-constrained urban maxima, and unit/rounding validation: 1) Inventory inputs and log absolute file paths, product versions, acquisition dates/times (validate daytime acquisition consistent with the analysis intent), and tool/code versions for ASTER TIR bands (10–14), the AOI polygon (e.g., Paris), and ancillary masks (cloud/water/land/urban). 2) Derive LST using the Two-Channel/Three-Temperature Method (TTM) on ASTER Bands 10–12; document the exact algorithm, constants/coefficients, and tool versions; confirm the LST output units are Kelvin. 3) Clip the LST raster to the AOI; harmonize CRS, resolution, grid alignment, and extent; preserve nodata and carry forward an AOI-valid-data mask; use exact tool-returned file paths between steps for traceability. 4) Apply rigorous QC/masking before any statistics: exclude invalid/NODATA pixels and non-land using ancillary masks (cloud/water) and physical bounds (e.g., keep only 250–350 K); track the valid coverage fraction within the AOI. 5) Define urban and rural class masks with explicit thresholds and/or land-cover constraints: a) UHII-driven threshold approach—urban = within AOI AND LST strictly > 305 K; rural = within AOI AND outside the urban mask AND land-only AND LST ≤ 300 K; document operator strictness ('>' vs '≤'), ensure masks are disjoint, and report per-class valid pixel counts and coverage fractions. b) Domain-constrained approach—when the statistic must be limited to mapped urban areas, apply an explicit urban-area mask from a reliable product (e.g., GHSL, ESA WorldCover, Urban Atlas), reprojected/resampled and aligned to the LST grid; verify pixel alignment and document reprojection/resampling choices. 6) Verify plausibility: inspect LST histograms and summary statistics for the AOI and for each class; confirm ranges and distributions are reasonable; flag anomalies (unit/scaling issues, residual clouds/water). 7) Compute class-based mean LSTs using only valid pixels for each class; report means in Kelvin with pixel counts and optional uncertainty metrics (standard deviation or confidence intervals). 8) Compute UHII as the urban–minus–rural mean LST difference; report the signed magnitude in Kelvin and the date/scene metadata; optionally provide robustness checks (median-based UHII or trimmed means). 9) Domain-constrained urban extremum (max) for specific reporting: when asked for an urban-only maximum (e.g., to match a multiple-choice answer), compute the LST maximum (or a robust high percentile, such as the 99th) via zonal statistics strictly over valid urban pixels using the explicit urban-area mask and cloud/water QA masks; use the masked LST raster and exact tool-returned file paths; explicitly confirm the output units are Kelvin and document any rounding rules applied when selecting the closest option. 10) Ensure end-to-end reproducibility: perform QC before metrics, reuse exact output file paths between stages to avoid inconsistent inputs, and log AOI definitions, CRS/resampling choices, mask sources (cloud/water/urban), thresholds, operators, and any processing changes.",
    "13": "Robust EO small-object detection pipeline (aircraft/tanks) with tiling, NMS, verification-driven fallback selection, deterministic post-processing, structured outputs, validation, and tool-use constraints: 1) Inventory inputs and metadata: enumerate images deterministically (e.g., get_filelist) and log absolute file paths, sensor/platform, resolution, acquisition date/time, orthorectification status, CRS, and AOI definition; record code/tool versions, model checkpoints, configuration files, and random seeds. 2) Use specialized detectors for aerial/satellite small objects: select models trained on relevant datasets (e.g., DOTA, xView) such as YOLOv5/v8, RetinaNet, Faster R-CNN/Detectron2 with FPN/tiling-aware anchors; do not rely solely on generic segmentation tools (e.g., SAM) for object detection; document model class ontology (e.g., Aircraft, Tank), label mappings, and calibration status. 3) Preprocess and tile large images: verify georeferencing and pixel resolution; tile into chips with defined size and overlap/stride; preserve tile indices and georeferencing so detections can be mapped back; normalize inputs (bands, radiometric scaling) as required by the detector; document tile size, overlap, and any resampling. 4) Run detection with explicit parameters and verification-driven tool-use: set and document confidence thresholds, NMS type (class-wise vs class-agnostic), IoU thresholds, and max detections per tile; pass exact returned file paths from the tiler to the detector. Attempt the detector first; if the step fails or yields zero valid candidates after QC, retry once. On persistent failure or low-confidence outcomes, fall back to a segmentation-based approach with a prompt (e.g., SAM/interactive segmentation) and request multiple candidate masks per target (top-k) to reduce reliance on a single prompt outcome; document prompt strings/parameters, seeds, and max candidates; keep runs deterministic. 5) Merge tile-level outputs: convert per-tile detections to scene coordinates and CRS; apply a global NMS/soft-NMS to remove cross-tile duplicates; carry forward class labels, confidences, and tile provenance; verify geometries (boxes/polygons) and coordinate validity. 6) Deterministic post-processing and candidate selection: validate candidate appearance with simple brightness/HSV thresholds when a color cue is required; document threshold values and bands/color-space used. Confirm image orientation (north-up) and coordinate conventions: pixel vs georeferenced coordinates, axis directions (e.g., x increases east; y decreases north in image array vs increases north in map CRS). Rank candidates objectively according to the request: northernmost = minimal centroid y (north-up); westernmost = minimal centroid x (map coordinates); use largest-by-mask area as a secondary tie-breaker. Compute centroids from masks using image moments in georeferenced coordinates when available; prefer mask area over bbox area; only use bbox centers/areas when masks are not available. Record thresholds, orientation/convention checks, ranks, areas, and centroid computations in the decision trail. 7) Parse outputs into structured counts and handle anomalies programmatically: clip detections to the AOI; compute per-scene and per-AOI counts; sanitize outputs (drop/flag NaNs/Inf, out-of-bounds indices, invalid geometries, negative/zero areas, missing fields); ensure centroid coordinates fall within AOI/image bounds and convert consistently between pixel and map coordinates; deduplicate identical detections; record counts, confidence distributions, and, if needed, sensitivity to threshold sweeps. 8) Validate via overlays and spot checks: render overlays of detections and selected candidates on imagery (quicklook PNGs/GeoJSON); perform stratified spot checks (random tiles, low/high confidence); optionally cross-validate with a second detector/segmenter and reconcile disagreements; where ground truth exists, compute basic metrics (precision/recall/F1) and document discrepancies. 9) Aggregate across scenes/dates: build time-stamped counts, densities (per km² or grid), and summaries; define and document rules for deduplicating persistent objects across closely spaced timestamps; report uncertainties and coverage fractions. 10) Ensure end-to-end reproducibility: chain all stages via exact returned file paths; retain AOI/CRS/resolution/extent consistency; log parameters, model checkpoints, configs, thresholds (including brightness/HSV), NMS settings, retries/fallbacks, prompt parameters and candidate counts, orientation/coordinate-convention checks, ranks/areas/centroids, and any retries/fallbacks; avoid ad hoc edits without documented rationale. 11) Export interoperable outputs: save geospatial detections (GeoJSON/COCO/CSV) with scene-level metadata, CRS, confidence, provenance, and post-processing selections (e.g., northernmost/westernmost, largest-by-mask, centroid coordinates); include processing logs to enable full auditability.",
    "14": "Reproducible single-channel TIR lake surface temperature workflow: 1) Inventory and verify metadata: log absolute file paths, sensor/platform, collection/version, acquisition date/time, processing date, product level, and units (DN vs radiance vs brightness temperature vs surface temperature). Record rescaling coefficients (ML/AL) and K1/K2 constants for the thermal band(s). 2) Apply the correct thermal chain: if inputs are DN, convert to TOA radiance (L = ML × DN + AL); convert radiance to brightness temperature (BT = K2 / ln(K1/L + 1)) and confirm BT is in Kelvin; derive surface temperature via a single-channel algorithm requiring emissivity and atmospheric parameters (transmittance, upwelling/downwelling radiance). Document the exact algorithm variant, constants, sources, and tool versions. 3) Emissivity handling for water: use an appropriate water emissivity (typically 0.98–0.99); avoid NDVI-based emissivity for pure-water pixels; optionally source emissivity from ASTER GED or a vetted emissivity library; document emissivity values/definitions. 4) Atmospheric parameters: obtain transmittance and path radiance from a consistent source (e.g., MODTRAN runs, NCEP/ERA5 profiles via atmospheric correction tools, or rely on sensor Level-2 Surface Temperature products when available); record data sources, time-matching, and any view-zenith dependence. 5) Constrain to the lake AOI: clip all rasters to the lake polygon; optionally erode the shoreline buffer to reduce mixed pixels; harmonize CRS (prefer equal-area for area-based stats), resolution, grid, and extent; confirm pixel alignment across inputs. 6) QA/cloud/ice masking: apply sensor-specific QA masks (e.g., Landsat QA_PIXEL/BQA bits for cloud, cloud shadow, snow/ice, and fill; Sentinel-2 QA60/SCL; MODIS SR QA/state bits); carry forward a per-scene valid-data mask and preserve nodata; track valid coverage fraction over the lake AOI. 7) Water-only constraint: intersect the valid-data mask with the lake polygon; exclude non-water classes consistently across dates; document mask sources and parameters. 8) Compute robust per-scene lake ST statistics: use only valid lake pixels to derive median and a trimmed mean (e.g., 10% trim), along with spread metrics (IQR or standard deviation); report pixel counts and valid coverage; flag low-confidence scenes when coverage is below a set threshold. 9) Sanity checks and external validation: inspect ST ranges/histograms for plausibility (e.g., seasonal ranges ~273–310 K depending on climate), and cross-validate against external SST/lake temperature products or climatology (e.g., GHRSST, Copernicus lake surface temperature, ERA5 lake skin temperature, in-situ buoy data); investigate discrepancies (units, emissivity, atmosphere, masking, misalignment) and document resolutions. 10) Ensure end-to-end reproducibility: retain AOI definitions, CRS/resampling choices, exact algorithm parameters (ML/AL, K1/K2, emissivity values, atmospheric inputs), QA masks applied, per-scene statistics with pixel counts and coverage fractions, and exact input/output paths; chain steps via returned file paths and document any processing changes.",
    "15": "Reproducible ship-detection pipeline with water prefiltering, robust counting, validation, and tool-use constraints: 1) Inventory and normalize inputs: deterministically enumerate images for the maritime AOI (e.g., ports/coastal corridors); log absolute file paths, sensor/platform, resolution, acquisition dates/times, CRS, and AOI definition; record code/tool versions, model checkpoints, configs, and random seeds. 2) Preprocess to isolate water and valid pixels: compute NDWI/MNDWI on surface reflectance or use a vetted water layer; apply sensor-specific QA/cloud/shadow masks; optionally erode shoreline buffers to reduce mixed pixels; carry forward a valid-water mask with coverage fractions; preserve nodata and record exact paths. 3) Run a primary ship detector: select a detector trained/tuned for ships (e.g., YOLOv5/v8, RetinaNet, Faster R-CNN with maritime datasets); tile large scenes with defined chip size/overlap; document detector thresholds (confidence), NMS type and IoU, max detections per tile; constrain detections to the valid-water mask (filter out land detections). 4) Extract counts robustly: for box outputs, perform NMS/soft-NMS and deduplicate across tiles; for segmentation/mask outputs, use connected components with size/shape filters (min/max area, elongation/length-to-width ratio) to suppress false positives; sanitize geometries (drop NaNs/Inf, invalid/negative areas) and record per-scene counts and confidence distributions. 5) Validate quickly: render overlays of detections on imagery (quicklook PNGs/GeoJSON); spot-check random tiles and high/low confidence cases; optionally cross-validate with a second detector; document disagreements and adjustments. 6) Parse tool outputs defensively: require structured fields (class, confidence, geometry, tile provenance); enforce confidence thresholds and margins; clip detections to AOI and valid-water; maintain decision trails for filtered/kept detections. 7) Respect tool-use constraints and implement fallbacks: if a step fails, retry once using the returned paths; on persistent failure or low-confidence outcomes, switch to a robust fallback (e.g., segmentation-based counting with connected components and domain-informed filters) and log the fallback choice; keep runs deterministic with fixed seeds. 8) Save intermediate artifacts for auditability: water masks, tiling outputs, raw detector results, NMS/connected-components outputs, overlays, and final counts with exact file paths; export interoperable detections (GeoJSON/CSV) with scene metadata, CRS, confidence, and provenance. 9) Aggregate and report: compute per-scene and time-window counts/densities (per km² of valid-water), coverage fractions, and uncertainty summaries; document AOI definitions, masks, thresholds, model/config versions, and any processing changes to ensure end-to-end reproducibility.",
    "16": "Reproducible LST exceedance pipeline with explicit day filtering, 40% threshold semantics, monthly aggregation, and year-to-year comparison: 1) Inventory inputs via a deterministic file listing and log absolute file paths, sensor/platform (Terra/Aqua), collection/version, acquisition date/time, and all ancillary layers (BT31/BT32 or LST product, emissivity source such as ASTER GED or NDVI-based, QA/cloud/smoke masks, water mask); record AOI definition and code/tool versions. 2) Filter to daytime scenes consistent with urban heat intent (Terra ~10:30 local, Aqua ~13:30) and to the exact ROI; document the chosen platform/time and rationale. 3) Clip all inputs to the ROI and harmonize spatial properties: choose an appropriate CRS, co-register and align rasters to a common grid, resolution, and extent; document resampling methods and verify pixel alignment. 4) Derive LST with a documented method: prefer split-window using BT31/BT32 with per-pixel emissivity; if split-window is unavailable, use single-channel from BT31 with emissivity; alternatively use the sensor LST product. Confirm input BT units (Kelvin), emissivity definitions/values, algorithm/coefficients, and ensure the LST output is in Kelvin; record formulas and parameters. 5) Apply QA/cloud/smoke/water masking before any exceedance metrics: use MODIS cloud/state bits (e.g., MOD35/MYD35 or SR QA) to remove clouds, cloud shadows, heavy aerosol, smoke, snow/ice, and fill; exclude water if analyzing land heat; carry forward a per-scene valid-data mask and track the ROI coverage fraction. 6) Clarify the \"40% threshold\" by separating two concepts and documenting operators: coverage threshold Cmin (e.g., require valid-data coverage ≥ 40% of the ROI for a scene/month to be included) and exceedance threshold Texceed (e.g., strictly > 310 K for LST). State whether operators are strict ('>') or inclusive ('≥'). 7) Compute per-scene exceedance percentage using only valid pixels: numerator = count of pixels with LST > Texceed; denominator = total valid pixels; proportion p_scene = numerator / denominator; record pixel counts and the valid coverage fraction per scene; flag scenes failing Cmin. 8) Aggregate to monthly exceedance with pixel-weighted averages: group scenes by calendar month; compute p_month as the weighted mean of p_scene using valid pixel counts as weights (equivalently, sum numerators across scenes divided by sum denominators); record weights, coverage fractions, and exclude scenes/months failing Cmin with reasons logged. 9) Compare years transparently: aggregate monthly exceedance into yearly summaries (e.g., weighted mean or median), report scene/month counts and coverage; compute year-to-year differences in magnitude (not just sign), reconcile with candidate options by documenting the exact values, and avoid selecting answers solely by sign. 10) Validate and sanity-check: inspect LST ranges/histograms over valid land pixels; cross-check exceedance levels against independent LST products (MOD11/MYD11 or MOD21/MYD21) or ERA5; verify that thresholds and masks behave as expected; investigate anomalies (unit issues, misalignment, QA leakage). 11) Ensure end-to-end reproducibility: retain AOI definitions, CRS/resampling choices, day/night selection, emissivity sources and parameters, QA/smoke/water masks applied, thresholds and operator strictness, per-scene counts and coverage fractions, monthly aggregation tables, yearly summaries and differences, and exact input/output file paths; chain outputs via returned paths and document any processing changes.",
    "17": "Reproducible residential built-volume time series and trend workflow: 1) Define the target variable precisely: residential_volume = built_volume_total − built_volume_nonres; confirm consistent units (e.g., m³ per pixel) across inputs. 2) Inventory rasters via get_filelist and confirm year coverage for the exact analysis window (e.g., 1985–2020); log absolute file paths, years, product names/versions, and processing dates. 3) Harmonize inputs: reproject to a common CRS and align resolution, grid, and extent; apply consistent masks (nodata/QA/urban AOI) and track valid coverage fractions per year. 4) Compute residential rasters via per-pixel raster subtraction; save outputs and record paths for traceability. 5) Aggregate to city-wide totals using sum over valid AOI pixels per year (calc_batch_image_sum); record per-year pixel counts, masked fractions, and totals. 6) Build the annual series strictly over the requested years, handling missing years explicitly (gap markers or exclusions with reasons). 7) Fit a linear trend on the annual totals using compute_linear_trend; report slope, intercept, R², p-values, standard errors, and confidence intervals; include basic diagnostics and note assumptions. 8) Verify units throughout and apply any necessary scaling (e.g., pixel-area conversions) transparently; retain native units in stored outputs to avoid confusion. 9) Ensure end-to-end reproducibility: chain steps via exact returned file paths, document AOI definitions, CRS/resampling choices, masks, subtraction/sum operations, tool versions/configs, and any processing changes.",
    "18": "Reproducible VIIRS nighttime lights seasonal (Jan–Apr) city comparison workflow with stable-lights/quality gating: 1) Enumerate and group Jan–Apr files per year via a deterministic listing; log absolute file paths, product names, platform (SNPP/NOAA-20), collection/version, processing dates, and radiance units (e.g., nW/cm²/sr). Prefer monthly composites such as NASA Black Marble (e.g., VNP46A3) or NOAA VIIRS stable lights (VCMSL), and record whether stray-light correction is included. 2) Validate product type and units: confirm radiance vs DN, scale factors, and the presence of stability masks/flags (ephemeral lights, snow/ice, lunar/stray light). Document processing versions to ensure cross-year comparability. 3) Constrain to the city AOI: clip all images to the city polygon; harmonize CRS (use an equal-area CRS for area-weighted stats if needed), resolution, grid, and extent across years; record resampling/interpolation methods and verify pixel alignment. 4) Apply quality/consistency masks before statistics: use product QA layers to mask clouds, snow/ice, lunar illumination/stray light, and ephemeral sources; optionally exclude water with a reliable mask; carry forward a per-image valid-data mask and track AOI coverage fraction. 5) Enforce stable-lights/thresholds: when using non-stable products, apply an AOI-specific radiance threshold and/or stability flags to suppress ephemeral lights (fires, gas flares, aurora, holiday lights) and snow artifacts; document threshold values, operators ('>' vs '≥'), and rationale; keep thresholds consistent across years. 6) Compute per-image zonal statistics on valid pixels: mean and median radiance over the AOI; record pixel counts, valid coverage fractions, and quick histograms to sanity-check ranges and unit consistency; flag low-confidence images with poor coverage. 7) Aggregate to a seasonal (Jan–Apr) summary per year: compute the pixel-count-weighted mean of the per-image means (or directly sum radiance numerators over valid pixels divided by total valid pixels across included images); also report variability (standard deviation and/or IQR); log included months, weights, and excluded images with reasons (e.g., coverage < threshold). 8) Compute change metrics: absolute change = seasonal_mean_year2 − seasonal_mean_year1; percent change = 100 × (absolute change / seasonal_mean_year1); report alongside variability and the number of valid images per year to contextualize uncertainty. 9) Consistency and cross-validation: verify processing versions and correction status (stray light/snow) are comparable across years; if versions differ, reprocess or note comparability limits; optionally cross-validate trends with independent sources (e.g., DMSP-OLS, alternate VIIRS products) or proxies (electricity usage, built-up area changes). 10) Ensure end-to-end reproducibility: retain AOI definitions, CRS/resampling choices, QA/stability masks, thresholds/operators, per-image stats, seasonal aggregates, change metrics, coverage fractions, and exact input/output paths; avoid ad hoc edits without documented rationale."
  },
  "config": {
    "exp_id": "earth_agent_practice_gpt5",
    "practice": {
      "epochs": 1,
      "batch_size": 10,
      "grpo_n": 3,
      "rollout_concurrency": 8,
      "rollout_temperature": 1.0,
      "task_timeout": 600,
      "num_experiences_per_query": 1,
      "given_ground_truth": true,
      "do_eval": false,
      "eval_strategy": "epoch",
      "eval_steps": 10,
      "eval_data_truncate": null,
      "shuffle_data": true,
      "rollout_data_truncate": 60,
      "restart_step": null,
      "agent_objective": "Input: An Earth observation question with satellite/remote sensing data.\nOutput: A reasoned answer using appropriate geoscience tools for data analysis.",
      "learning_objective": "Extract generalizable strategies for Earth observation analysis: which tools to use in which order, how to interpret geospatial results, and how to combine multiple data sources."
    },
    "model": {
      "model_name": "gpt-5",
      "api_key": "sk-1LxurljHwVSWdBGEYWBJq68VRHRQuuPbQrIHOOBpsybG78YV",
      "base_url": "http://35.220.164.252:3888/v1/",
      "temperature": 1.0,
      "max_tokens": 8192,
      "timeout": 120
    },
    "judge_model": {
      "model_name": "gpt-5",
      "api_key": "sk-1LxurljHwVSWdBGEYWBJq68VRHRQuuPbQrIHOOBpsybG78YV",
      "base_url": "http://35.220.164.252:3888/v1/",
      "temperature": 0.3,
      "max_tokens": 4096,
      "timeout": 120
    },
    "practice_dataset_path": "../benchmark/question.json",
    "eval_dataset_path": null,
    "question_ids": [
      "10",
      "114",
      "137",
      "176",
      "2",
      "215",
      "23",
      "3",
      "53",
      "102",
      "116",
      "138",
      "186",
      "202",
      "218",
      "231",
      "30",
      "55",
      "107",
      "119",
      "149",
      "187",
      "203",
      "223",
      "233",
      "33",
      "8",
      "109",
      "12",
      "155",
      "19",
      "208",
      "224",
      "236",
      "4",
      "80",
      "11",
      "121",
      "161",
      "190",
      "209",
      "225",
      "242",
      "40",
      "110",
      "125",
      "17",
      "191",
      "21",
      "228",
      "244",
      "43",
      "112",
      "136",
      "173",
      "195",
      "210",
      "229",
      "28",
      "45"
    ],
    "output_dir": "./training_free_results",
    "log_dir": "./training_free_logs",
    "verify_module": "earth_science",
    "langchain_config_path": "../agent/config_gpt5.json",
    "use_autoplanning": true
  },
  "timestamp": "2025-11-22T15:02:08.801996"
}