{
  "0": "Structured candidate-selection pipeline for attribute-qualified object selection (e.g., tanks or ground track fields): 1) Enumerate and load the image; record exact paths and confirm georeferencing/orientation—correct to north-up when not georeferenced, or use projected coordinates so west corresponds to smaller x and north to smaller y. 2) Detect/segment all candidate instances: run the primary detector; if detections are insufficient, retry once; if still insufficient, fall back to promptable segmentation (e.g., RemoteSAM/GroundedSAM) and classical CV methods (e.g., luminance/brightness thresholding for “white” targets, Hough circles and circularity metrics for tank-like shapes); tile large scenes as needed. 3) Convert masks to polygons/bounding boxes; validate geometries and compute per-instance attributes: mask area (prefer mask area over bbox) and centroid from the mask/polygon (not the bbox center). 4) Enforce spatial qualifiers quantitatively: select “largest” by maximum mask area; select “northernmost” by minimal y; select “westernmost” by minimal x after confirming axis orientation/georeferencing (use projected eastings/northings to avoid axis confusion when available). 5) Attribute validation: confirm class-specific cues (e.g., “white” via luminance thresholds) before applying spatial qualifiers. 6) Robust parsing and QA: handle NaNs, verify bbox extents and mask integrity, deduplicate overlaps, and ensure consistent path handling across tools. 7) Validation: overlay candidates and final selections on imagery for visual QC. This detect-then-filter approach avoids relying on a single prompt to satisfy spatial qualifiers and improves accuracy under multiple constraints.",
  "1": "Disciplined, reproducible EO LST workflow with metadata-driven thermal conversions, physically based retrieval, class/mask-constrained statistics, multi-zone classification, threshold-based proportions, and time-of-day handling (with a landcover-specific comparison protocol): 1) Inventory inputs and metadata: enumerate files and record exact paths returned by tools; capture sensor/product versions; confirm the retrieval method (single-channel vs split-window) and verify the appropriate coefficients for the chosen algorithm (document source/version); confirm variables/bands needed (e.g., Landsat TIRS B10; MODIS BT31/BT32), units, scale factors, CRS, nodata/QA flags, and band-to-band coregistration. For Landsat, read the MTL to determine DN vs radiance vs BT and extract ML/AL and K1/K2. Confirm availability of emissivity inputs and, if required, atmospheric parameters. Prefer physically retrieved LST over raw brightness temperature and confirm target LST units are Kelvin. 2) Preprocess and QA masking: apply QA/cloud/shadow (and smoke/ice/snow when relevant) masks and land/water masks; explicitly exclude water using QA, NDWI, and/or land-cover where applicable; remove fill values and apply scale factors; clip strictly to the AOI. Persist explicit input and output file paths after each stage so downstream tools receive exact paths for reproducibility. 3) Geometry harmonization: align/resample all inputs and masks to a common analysis grid; verify and enforce co-registration (projection, resolution, extent, and pixel alignment) across BT31/BT32, emissivity, ancillary masks, and AOI. Correct any pixel shifts before analysis. 4) Thermal conversion and LST computation: if starting from DN, convert to at-sensor radiance (Lλ = ML*DN + AL) then to brightness temperature (BT = K2 / ln(K1/Lλ + 1)); if BT/LST is provided, validate units (Kelvin) and skip redundant steps. Compute LST with physically based methods: (a) Single-channel/mono-window on Landsat Band 10 (or ETM+ Band 6) using NDVI-based emissivity and documented atmospheric terms; or (b) Split-window on dual thermal bands (e.g., MODIS Bands 31/32) using documented and verified coefficients and emissivity assumptions. Set emissivity appropriately (e.g., water ≈ 0.99; land via NDVI/EVI or class-based values). Verify outputs are Kelvin and physically plausible via summary checks. 5) Class-constrained analysis (optional): when summaries target a class (e.g., forest vs non-vegetated), apply authoritative masks (e.g., ESA WorldCover, Copernicus GHSL/GHS-BUILT); reproject/resample masks to the sensor grid. If authoritative masks are unavailable, use justified proxies and document thresholds/rationale—e.g., compute NDVI and define forest as NDVI > 0.7 and non-vegetated as NDVI < 0.2; always exclude water/cloud/NoData unless water is the target. 6) Zonal statistics, multi-zone classification, and threshold-based proportions: compute robust statistics (e.g., medians, p95) per class mask over valid pixels; for distributional reporting, classify LST into all requested temperature zones (e.g., user-specified intervals) and compute per-class area percentages. Use only valid LST pixels (post-QA and within AOI) as the denominator; perform area-based proportions in an equal-area CRS or via per-pixel area weights. Document all thresholds/zone boundaries and report the valid-pixel denominator/area. 7) Differences and definitions: define and document whether differences are signed or absolute and apply consistently. Align the sign with the question (e.g., non-vegetated mean − forest mean so positive indicates forest is cooler). 8) Temporal and time-of-day handling: ensure time filtering matches the request; if day/night is ambiguous, process both and justify the final choice based on the question’s intent. Aggregate temporally using only valid pixels and weighting by valid pixel counts; convert Kelvin to Celsius only at the reporting stage if needed. 9) Validation and sanity checks: inspect BT/LST histograms, ranges, and spatial patterns; confirm LST magnitudes are physically plausible and distinct from raw BT; verify hotspot/coolspot locations; validate saved outputs and catch unit/scale errors. 10) Provenance and reproducibility: record inputs, versions, MTL-derived parameters (ML/AL, K1/K2), split-window/mono-channel coefficients, emissivity and atmospheric terms, masks, QA criteria, grids, thresholds/zone boundaries (including NDVI/NDWI class cuts), intermediate statistics, and all exact input/output paths passed between tools so results are traceable and reproducible.",
  "2": "Verify requested variable availability and perform robust trend analysis with an interpretable time axis: 1) Confirm the dataset contains the target geophysical variable via filenames/variable names, metadata (units, scale/offset, fill values), and QA flags; if absent, identify and source an appropriate dataset (e.g., precipitation from GPM/IMERG, ERA5; or compute indices like SPI). 2) Preprocess: apply QA/cloud masks; handle/remove fill values and apply scale factors; harmonize spatial/temporal coverage. 3) Temporal aggregation and coverage audit: parse timestamps, sort chronologically, and aggregate to the requested temporal resolution (e.g., monthly for Sep–Dec); confirm the requested temporal coverage and explicitly document gaps/missing periods (e.g., a missing 2024). 4) Define and center the time axis before regression: build an explicit date/year-to-mean mapping for the aggregated series and use a time index starting at 0 (t = year − baseline_year or t = 0 for the first aggregated time step) so the intercept reflects the baseline period’s mean; explicitly verify chronological ordering and persist the date↔t mapping to avoid confusing negative intercepts from using absolute years and to keep results interpretable (e.g., for option comparison); for irregular cadences use decimal years or sequential indices. 5) Assess trends using both parametric and non-parametric methods: run OLS regression on the aggregated series using t as predictor (report slope in units per year and R²; interpret intercept as the baseline mean), and run Mann–Kendall with Sen’s slope (report direction and statistical significance).",
  "3": "Robust EO inter-city/area comparison workflow with strict AOI/time matching and period-level percent-change reporting: 1) File inventory and metadata QA: enumerate files and record exact paths; verify CRS, spatial resolution, temporal coverage, units, scale factors, nodata values, cloud/quality masks; confirm product comparability across comparison groups (product/version, radiometric calibrations, QA definitions) to avoid mixed datasets. 2) Build clean, scope-aligned datasets: programmatically filter inputs to the exact AOI and requested period(s) (e.g., months Jan–Apr by year) to create two clean datasets; if coverage is incomplete or missing, pause to report the mismatch and/or locate alternatives before proceeding. 3) Valid-data and domain masks: apply QA/cloud/snow masks; apply the AOI and, when relevant, an additional domain/interest mask (e.g., lit/urban areas for nighttime lights) to focus analysis on appropriate pixels; exclude nodata/water where not targeted. 4) Geometry harmonization: align projections/resolution/grids and compute zonal statistics strictly on valid pixels. 5) Per-date/month statistics: compute per-date AOI metrics and then robust per-month statistics (median and mean) with variability (e.g., IQR/standard deviation) and valid pixel counts. 6) Date alignment: align dates/months between comparison groups so statistics are based on matched acquisitions to avoid sampling bias; document any gaps and their handling. 7) Period aggregation: aggregate the selected months to a period-level summary (e.g., Jan–Apr composite) using weights based on valid pixel counts; convert Kelvin to Celsius only at the reporting stage if needed. 8) Differences and percent change: compute signed differences between groups/years and percent change relative to a clearly defined baseline; state the sign convention and denominator; report variability/uncertainty (e.g., standard deviation and standard error across matched dates/months). 9) Sanity checks: inspect value ranges, histograms, outliers, seasonality, and spatial coverage; confirm expected magnitudes and that masks were applied correctly. 10) Documentation and transparency: record inputs, versions, processing parameters, masks used (including domain masks), matched-date lists, grids, thresholds/filters, intermediate stats, and outputs to support reproducible, bias-resistant comparisons.",
  "4": "Temporal ATI pipeline for August California: 1) Enumerate all August day/night brightness temperature files and matching surface albedo files for the target sensors/year. 2) Pair same-day day and night BT observations; match each pair to the correct albedo for the same date/time (or nearest valid within an acceptable window). 3) Apply QA masking for clouds, smoke, bad quality flags, and nodata; optionally exclude saturated or unreliable pixels near active fire fronts. 4) Clip strictly to the California AOI; align projections/resolution/grids across BT and albedo; document any resampling method used. 5) Compute ATI per pixel for every valid paired date using the project’s defined ATI formula; filter out implausible values based on range checks. 6) Build a per-pixel temporal composite (e.g., median ATI) for August to reduce noise; alternatively, compute area-weighted daily ATI ratios based on valid pixel counts. 7) Compute the regional metric: percentage of pixels with composite ATI < 0.4 within the AOI and valid mask, using an equal-area CRS for area-based proportions. 8) Validate with histograms, distribution/range checks for BT, albedo, and ATI; include spatial sanity checks and sensitivity to the ATI threshold. 9) If the question targets wildfire events, restrict dates/areas using active fire perimeters or hotspot data (e.g., FIRMS/CalFire) so the statistic aligns with event periods. 10) Maintain provenance: product versions, dates, masks, parameters, and pairing/selection criteria to ensure reproducibility.",
  "5": "NDWI water detection QA/QC pipeline: 1) Validate NDWI availability and scaling: confirm index values are within [-1, 1]; if NDWI is absent or improperly scaled, recompute from the correct bands with appropriate scale factors. 2) Preprocess: apply QA/cloud and shadow masks; clip precisely to the AOI (e.g., lake polygon) and align projection/resolution/grids across inputs. 3) Use a consistent, justified NDWI threshold for water detection; document parameter choices and any sensor-specific adjustments. 4) Ensure traceability: use exact file paths returned by tools and subset strictly to dates relevant to the question. 5) Summarize results with robust statistics (e.g., medians or percentiles rather than raw min/max); compute area-based proportions within valid masks using an equal-area CRS, reporting the valid-pixel denominator/area. 6) Base final interpretation only on these quality-controlled, scope-aligned metrics; perform range and spatial sanity checks before reporting.",
  "6": "Confidence-aware, taxonomy-aligned multi-stage scene classification workflow (with explicit ambiguity handling and geoscience corroboration): 1) Inventory inputs and metadata: record sensor/platform, date/time, CRS, modality, and exact file paths; define the required output taxonomy and ensure the primary classifier’s label space aligns to the task. 2) Prepare auxiliary signals: if multispectral bands exist, confirm band names/scale factors so NDVI/EVI/NDWI can be computed; optionally stage domain layers (e.g., DEM, land/water masks) for corroboration. 3) Run the primary classifier and capture full outputs: top-1 predicted class, top-1 confidence, and the full top-k probability vector (at least top-5). 4) Robust parsing: handle NaNs/missing scores, normalize probabilities if needed, map class IDs to standardized labels, verify per-input output shape/completeness, and retain top-k for auditability; ensure strict adherence to any required final output schema/format. 5) Stage-1 accept/reject thresholds: require a minimum top-1 confidence and a minimum margin vs top-2; define a borderline band (e.g., top-1 probability between 0.4–0.6 or small margin). Accept only when thresholds are met; otherwise flag as ambiguous. 6) Ambiguity detection using top-5 and known confusion sets: explicitly flag images where the top-5 includes common confusions in the taxonomy (e.g., Farmland vs Meadow/BareLand/Desert; Park vs Meadow/Playground/Square; Beach vs Harbor/Sandbar); maintain a mapping table of confusion sets to drive targeted checks. 7) Stage-2 corroboration for flagged images with geoscience cues: perform secondary checks—(a) quick visual inspection for obvious cues; (b) a second model or ensemble vote; (c) simple feature-based heuristics and domain indicators such as vegetation/green fraction (from RGB or NDVI/EVI), field-pattern textures/regular edges, parcel grids, irrigation circles/center-pivot patterns for Farmland, rectilinear path networks, and water adjacency via NDWI; for coastal cases, use coastline geometry cues. Document thresholds/parameters and outcomes. 8) Decision policy and abstention: decide the final class per scene with a documented policy. Count a scene as the target class only when top-1 matches and confidence ≥ threshold, or when secondary checks corroborate a borderline case; abstain when unresolved and record reasons. 9) Aggregation and reporting: aggregate accepted classifications to requested units; compute counts and percentages by class using only accepted scenes; track abstained/uncertain cases and optionally produce confidence-weighted summaries; when required, strictly map the final tally to the specified output schema (e.g., a choice letter) and validate output format. 10) Validation, compliance, and provenance: perform schema validation, label-domain checks, confidence range checks, and spot audits with overlays; log model versions, confusion-set mappings, thresholds (including class-specific cutoffs), domain cues used (e.g., irrigation circles, field-pattern textures), secondary models invoked, tie-break logic, and aggregation/output-format conversions so results are traceable and reproducible.",
  "7": "Vegetation coverage change-detection pipeline aligned to a coverage metric: 1) Inventory and sort NDVI files by date; verify metadata (units, scale factors, CRS, nodata/QA flags) and convert scaled NDVI to physical units in [-1, 1]. 2) Preprocess: apply QA/cloud/shadow (and snow) masks; remove nodata/fill values; restrict to land pixels using a land/water mask; align all inputs to a common grid. 3) Define the vegetation coverage metric: (a) fractional vegetation cover (FVC) from NDVI using project-defined NDVIsoil and NDVIveg parameters (FVC = clip((NDVI − NDVIsoil) / (NDVIveg − NDVIsoil), 0–1)), or (b) percent of AOI area with NDVI ≥ a chosen threshold; compute area-based proportions only over valid pixels in an equal-area CRS and report the valid-pixel denominator/area. 4) Temporal aggregation: compute monthly robust statistics (e.g., median) for the chosen coverage metric to reduce scene-level noise; track valid pixel counts per month. 5) Change detection: compute consecutive percent changes in the monthly coverage metric; identify the maximum positive increase and report the two bounding months/dates and the magnitude; flag months with insufficient valid data. 6) Quality and sanity checks: confirm value ranges and histograms for NDVI/FVC; ensure comparisons use consistently masked AOIs; inspect spatial patterns for plausibility. 7) Documentation and transparency: record thresholds (NDVI threshold, NDVIsoil/NDVIveg), masks used, product versions, dates aggregated into each month, formulas, and any exclusions so results are reproducible and assumptions explicit.",
  "8": "Combined LST–vegetation dryness (e.g., TVDI) workflow with variable verification and crop masking: 1) Data audit: confirm the exact requested dates and variables; if EVI is specified, use EVI; if EVI is unavailable, explicitly justify NDVI substitution. Inventory product versions, units, scale factors, CRS, nodata values, and QA/bitmask definitions for both LST and VI. 2) Input inventory and verification: ensure LST and VI variables are present or computable; verify MODIS QA bitfields for LST and VI quality and cloud/shadow flags; check band-to-band/coregistration where relevant. 3) Preprocess: apply MODIS QA to remove invalid LST/VI pixels; remove fill values and apply scale factors; clip strictly to the AOI and the crop mask (e.g., Chengdu Plain rice areas); exclude water and other non-target classes as needed. 4) Harmonize geometry: reproject/resample LST and VI to a common analysis CRS, resolution, and grid; verify alignment/coregistration and correct any pixel shifts before index computation. 5) Vegetation index preparation: use EVI or NDVI per specification; if computing from bands, apply appropriate scale factors and confirm value ranges (e.g., NDVI/EVI within expected bounds). 6) TVDI derivation: compute the temperature–vegetation dryness index using the LST–VI feature space (e.g., dry/wet edges via triangular/trapezoid method); document the formula, parameterization, edge-fitting approach, and any AOI/time-specific coefficients; filter implausible TVDI values based on range checks. 7) Thresholding and area metrics: apply the given TVDI threshold; compute area-weighted percentages over valid pixels using an equal-area CRS or per-pixel area weights; report the valid-pixel denominator/area used. 8) Temporal aggregation (if required): aggregate per-date to monthly or seasonal summaries using only valid pixels and weighting by valid-pixel counts. 9) Validation and sanity checks: inspect histograms and ranges for LST, VI, and TVDI; check spatial patterns for plausibility within the crop mask; flag outliers or artifacts. 10) Provenance and reproducibility: persist explicit tool-returned file paths; record inputs, versions, masks, parameters, QA criteria, coefficients, and intermediate statistics so results are traceable and reproducible.",
  "9": "Baseline-anchored per-pixel increase assessment workflow (e.g., turbidity): 1) Define scope and baseline: identify the target variable (e.g., turbidity T), confirm its availability or derivation method, and select a defensible baseline per pixel (previous clear-water date, first valid observation, or project-specified reference). Document the baseline choice and logic. 2) Inventory and preprocessing: verify metadata (units, scale factors, CRS, nodata/QA flags) and apply QA/cloud/shadow and water masks; remove fill values; clip to the AOI; align/resample all inputs to a common analysis grid; restrict analysis strictly to valid water pixels. 3) Conversion/derivation: if turbidity is not directly provided, derive it from appropriate bands or products using documented formulas; apply scale factors and standardize units. 4) Compute per-pixel percent change: Δ% = (T_t − T_ref) / max(T_ref, ε), choosing a small ε to avoid divide-by-zero and masking pixels where T_ref is near zero or physically implausible; treat negative changes as non-increase unless otherwise required. 5) Classify severity: flag pixels as severe increases where Δ% > 30% (or a project-defined threshold); document the threshold and rationale; avoid tuning absolute thresholds to fit expected answers; optionally perform sensitivity checks. 6) Aggregate per date: compute the proportion of valid water pixels (or area-weighted proportion) classified as severe increase for each date; use an equal-area CRS or per-pixel area weights; report the valid-pixel denominator/area used. 7) Select the maximum: identify the date with the maximum severe-increase proportion and provide the full per-date series for transparency. 8) Validation and sanity checks: inspect percent-change histograms and ranges, map severe-change pixels to verify spatial plausibility, confirm masks applied correctly, and flag outliers/artifacts. 9) Provenance and reproducibility: record baseline selection rules, input paths/versions, conversions, masks, thresholds, parameters, grids, and intermediate statistics to ensure traceable, reproducible results.",
  "10": "Structured EO object-counting pipeline with comparability and uncertainty reporting: 1) Inventory and preview inputs/metadata: record sensor/platform, date, CRS, ground sample distance (GSD), image dimensions, orientation, AOI boundaries, and exact file paths; verify radiometric scaling and confirm coverage/extent. When planning to rank regions, explicitly verify that image scale/resolution and spatial extents are comparable or document differences. 2) Configure and run primary aerial detectors: choose detectors fine-tuned on overhead datasets (e.g., DOTA, xView, DIOR) with class mappings aligned to the target taxonomy; set per-class confidence thresholds, IoU thresholds, and NMS type (axis-aligned or rotated for OBB); tile large scenes with overlap and run multi-scale inference to capture small targets; optionally apply test-time augmentation. 3) Consolidate and filter detections: merge outputs across tiles/scales; apply cross-tile/scale NMS to de-duplicate; validate bbox/OBB coordinates within image bounds; filter by size/aspect ratio using GSD-based constraints; standardize class labels/IDs and retain confidences. 4) Fallback segmentation and counting: if detectors fail or have low recall, use instruction-guided instance segmentation (e.g., GroundedSAM/RemoteSAM with text prompts) or tailored classical segmentation; clean masks (opening/closing), count connected components, and apply size/shape filters (area, perimeter, circularity) to remove spurious components; keep per-component attributes. 5) Verification and cross-validation: create visual overlays of boxes/masks on imagery and perform spot checks; cross-validate counts with at least one alternate method (e.g., Hough circle-based detection for circular targets, a second YOLO/anchor-free model, or segmentation vs detection) and reconcile or flag discrepancies; if reference labels exist, estimate precision/recall. 6) Robust parsing and QA: handle NaNs/missing fields, validate output schemas/types, verify bbox/mask integrity and within-bounds coordinates, normalize probabilities if needed, and audit score/size distributions to catch anomalies. 7) Comparability normalization: compute counts per unit area (e.g., per km²) using an equal-area CRS or per-pixel area weights; restrict denominators to valid analysis area (e.g., within AOI and relevant masks) and record the exact valid-area used. Align acquisition dates/conditions across regions where feasible to reduce sampling bias. 8) Uncertainty summarization: quantify variability across methods and thresholds (e.g., range between detector and segmentation counts, confidence-weighted counts, standard deviation/standard error across tiles or dates); document tie-break policies and sensitivity checks. 9) Aggregation and reporting: compute per-image and per-region totals and densities; rank regions using normalized metrics; export tabular summaries (CSV/JSON) with counts, densities, valid-area denominators, uncertainty fields, and key metrics; optionally include confidence-weighted counts. 10) Provenance and reproducibility: log model versions, training datasets, thresholds, NMS settings, tiling overlap/stride, scales, prompts, size filters, AOI boundaries, area-calculation method, uncertainty metrics, and all intermediate outputs/overlays so results are traceable and rerunnable.",
  "11": "Modality-aware, validation-backed ship counting pipeline: 1) Inventory inputs and validate modality: enumerate files and confirm imagery type (optical vs SAR), sensor/product metadata (e.g., polarization for SAR, resolution, CRS, acquisition time), spatial coverage over water, and availability of shoreline/water masks; record exact file paths for traceability. 2) Primary, modality-matched detection with a single retry: run a ship-specific detector suited to the modality; if detections are insufficient, allow one retry with tuned thresholds/tiling; if still insufficient, pivot to an alternate detector. 3) Fallbacks if detectors fail: (a) Promptable segmentation with a \"ship\" prompt to obtain instance masks and count via connected components within the water extent; (b) Classical EO workflow: compute an NDWI-based water mask, apply intensity/texture thresholding within water, then morphological opening/closing and size/aspect-ratio filters to isolate ship-like objects. 4) Structured parsing and validation: prefer JSON outputs and validate schemas/types; handle NaN/Inf; verify bbox extents and mask integrity; attach per-object confidence and geometry (bbox, mask, centroid). 5) Post-processing and artifact control: apply confidence thresholds and non-maximum suppression (NMS); filter likely artifacts using shoreline/port/dock masks, distance-to-shore constraints, and clutter heuristics; deduplicate across tiles/overlaps. 6) QC overlays and spot-checks: generate quicklook overlays of detections on imagery for sampled tiles; log acceptance/rejection notes for auditability. 7) Cross-validation of counts: corroborate ship counts with at least one alternate method (e.g., detector vs. connected-components vs. classical); reconcile discrepancies with a documented tie-break policy. 8) Cross-modality reconciliation (when available): compare near-contemporaneous optical and SAR results to reduce false positives near docks and clutter; require consistency with water masks and plausible modality-specific signatures. 9) Counting and reporting: count unique ships after NMS; compute summary statistics (counts, densities) and optional centroids/areas; report valid-water area as the denominator for density metrics. 10) Provenance and reproducibility: record inputs, versions, parameters, thresholds, masks, QC overlays, and intermediate outputs so results are traceable and repeatable.",
  "12": "Transparent, consistent hotspot thresholding and annual aggregation workflow: 1) Year-level threshold derivation: compute the per-year mean radiance across all valid monthly images within the AOI; set a single year-level hotspot threshold as mean × 1.5 (or a documented project coefficient). 2) Threshold application options: (a) pass the fixed year-level threshold explicitly to the hotspot tool for every monthly image, ensuring uniform criteria; or (b) build an annual composite (e.g., mean/median radiance) first to reduce noise, then apply the threshold once to derive hotspots for the year. 3) QA/QC per month: mask nodata/invalid pixels using product QA flags; ensure consistent spatial coverage by aligning grids and restricting analysis to a common valid AOI; apply scale factors and verify units. 4) Monthly hotspot proportion: compute per-month hotspot proportions as the percentage of valid pixels exceeding the threshold (use equal-area CRS or per-pixel area weights); record the valid-pixel denominator. 5) Annual aggregation: aggregate monthly proportions to annual values using weights based on valid pixel counts (or area) per month; include only months with sufficient valid coverage. 6) Precision and variability: report clear rounding rules for the annual proportion; include variability (e.g., standard deviation across months) to convey uncertainty. 7) Consistency and reproducibility: avoid tuning thresholds to fit expected outcomes; document the threshold derivation (mean, coefficient), QA criteria, valid-pixel weighting, spatial coverage decisions, inputs/versions, and all intermediate statistics for traceability.",
  "13": "Repeatable aircraft spacing measurement pipeline: 1) Inventory and metadata: record sensor/platform, date/time, CRS, ground sample distance (GSD), image orientation (ensure north-up or note rotation), image dimensions, and exact file paths; verify units and any radiometric scaling. 2) Primary detection with targeted fallback: run aircraft-specific object detectors (trained/fine-tuned on aerial datasets); allow one retry with tuned thresholds/tiling/NMS; if still insufficient, pivot to adaptive thresholding within relevant brightness/texture ranges followed by morphological opening/closing and connected-component labeling. 3) Component filtering with priors: apply GSD-informed size (min/max area/length) and aspect-ratio constraints typical of aircraft planforms; validate bbox/mask coordinates within image bounds and reject spurious components. 4) Geometry and centroids: retain instance masks and attributes; compute centroids from the segmentation masks (not bbox centers); keep per-component IDs and coordinates in pixel units. 5) Nearest-neighbor measurement: compute pairwise or k-NN distances between centroids to identify the closest pair; convert pixel distances to meters using the GSD (and image rotation if not north-up); maintain strict unit consistency throughout. 6) QA and validation: handle NaNs/Inf in tool outputs, verify mask integrity and bbox extents, and confirm expected component counts; generate overlays showing detected aircraft, centroids, and the closest-pair linkage with annotated metric distances; avoid misaligned tools that lack positional outputs (e.g., skeleton/contour-only counts) for distance tasks. 7) Provenance and reproducibility: chain tool outputs with correct file paths; log detector versions, thresholds, morphological parameters, GSD used for conversions, distance computation method, and all intermediate artifacts so results are traceable and rerunnable.",
  "14": "Metadata- and semantics-driven metric selection for “total change”: 1) Inventory data and read raster metadata before any computation: record units, scale factors, CRS, resolution, nodata/fill values, pixel size/area, and exact file paths; confirm the geophysical variable and its units. 2) AOI consistency and masking: use a consistent AOI (e.g., Shanghai); apply water and nodata masks as appropriate to the question; align all inputs to identical grids/resolution/CRS to avoid sampling bias. 3) Let question semantics drive the metric: if the question asks for “total change,” compute sums across the AOI (not means); for averages or proportions, use means or threshold-based masks respectively; do not switch metrics to match expected option scales. 4) Unit handling and magnitude validation: convert to consistent physical units and use pixel area (from metadata or CRS-derived pixel dimensions) to validate and interpret magnitudes; prefer unit conversions and plausibility checks over ad hoc metric changes. 5) Baseline and change definitions: state the baseline explicitly (date/reference) and compute both absolute change and percent change relative to that baseline; define signed vs absolute differences and apply consistently. 6) Rounding and reporting: apply consistent rounding rules appropriate to the units and question; report the valid-pixel/area denominator and any exclusions. 7) Provenance and reproducibility: log inputs, versions, masks, grids, units/scale factors, conversions, baseline choice, metric selection rationale, thresholds, and output paths so results are traceable and defensible."
}